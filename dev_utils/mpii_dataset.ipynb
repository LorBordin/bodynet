{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3ec986-76e7-4230-a37b-4574d08123dc",
   "metadata": {},
   "source": [
    "# MPII dataset -- Preparation\n",
    "\n",
    "## 1. Download the dataset\n",
    "### 1.1 Download and extract the annotation files\n",
    "### 1.2 Selected the images of interest and the corresponding labels\n",
    "\n",
    "## 2. Image preparation\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1434e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f79d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import shutil\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from bodypose.dataset import save_keypoints\n",
    "from bodypose.dataset import create_TFRcords\n",
    "\n",
    "from config import MPII_KEYPOINT_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc097532-8667-473c-bb5d-1fd7d8011784",
   "metadata": {},
   "source": [
    "## 1. Download the dataset\n",
    "\n",
    "- Download the images from\n",
    "- Download the annlotations in json format from https://www.kaggle.com/datasets/harshpatel66/mpii-human-pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56816590-0471-4424-88a2-a80bab030cbf",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "### 2.1 Inspect the annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e950cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 25204 annotations.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../dataset/MPII/annotations/mpii_annotations.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(f\"The file contains {len(annotations)} annotations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873e14b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'MPI',\n",
       " 'isValidation': 0.0,\n",
       " 'img_paths': '015601864.jpg',\n",
       " 'img_width': 1280.0,\n",
       " 'img_height': 720.0,\n",
       " 'objpos': [594.0, 257.0],\n",
       " 'joint_self': [[620.0, 394.0, 1.0],\n",
       "  [616.0, 269.0, 1.0],\n",
       "  [573.0, 185.0, 1.0],\n",
       "  [647.0, 188.0, 0.0],\n",
       "  [661.0, 221.0, 1.0],\n",
       "  [656.0, 231.0, 1.0],\n",
       "  [610.0, 187.0, 0.0],\n",
       "  [647.0, 176.0, 1.0],\n",
       "  [637.02, 189.818, 1.0],\n",
       "  [695.98, 108.182, 1.0],\n",
       "  [606.0, 217.0, 1.0],\n",
       "  [553.0, 161.0, 1.0],\n",
       "  [601.0, 167.0, 1.0],\n",
       "  [692.0, 185.0, 1.0],\n",
       "  [693.0, 240.0, 1.0],\n",
       "  [688.0, 313.0, 1.0]],\n",
       " 'scale_provided': 3.021,\n",
       " 'joint_others': [[895.0, 293.0, 1.0],\n",
       "  [910.0, 279.0, 1.0],\n",
       "  [945.0, 223.0, 0.0],\n",
       "  [1012.0, 218.0, 1.0],\n",
       "  [961.0, 315.0, 1.0],\n",
       "  [960.0, 403.0, 1.0],\n",
       "  [979.0, 221.0, 0.0],\n",
       "  [906.0, 190.0, 0.0],\n",
       "  [912.491, 190.659, 1.0],\n",
       "  [830.509, 182.341, 1.0],\n",
       "  [871.0, 304.0, 1.0],\n",
       "  [883.0, 229.0, 1.0],\n",
       "  [888.0, 174.0, 0.0],\n",
       "  [924.0, 206.0, 1.0],\n",
       "  [1013.0, 203.0, 1.0],\n",
       "  [955.0, 263.0, 1.0]],\n",
       " 'scale_provided_other': 2.472,\n",
       " 'objpos_other': [952.0, 222.0],\n",
       " 'annolist_index': 5.0,\n",
       " 'people_index': 1.0,\n",
       " 'numOtherPeople': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in annotations:\n",
    "    if sample['numOtherPeople']==2:\n",
    "        break\n",
    "\n",
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9504c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coords(labels):\n",
    "    (H, W) = (labels['img_height'], labels['img_width'])\n",
    "    \n",
    "    c_kpts = np.array(labels['joint_self'])\n",
    "    c_kpts[:, :2] /= (W, H)\n",
    "\n",
    "    c_centers = [c_kpts[c_kpts[:, -1]==1].mean(axis=0)[:2]]\n",
    "    #if labels[\"numOtherPeople\"] == 1:\n",
    "    #        coords = np.array(labels[\"joint_others\"])\n",
    "    #        c_centers.append(coords[coords[:, -1]==1].mean(axis=0)[:2] / (W, H))\n",
    "    #elif labels[\"numOtherPeople\"] > 1:\n",
    "    #    for joints in labels[\"joint_others\"]:\n",
    "    #        coords = np.array(joints)\n",
    "    #        c_centers.append(coords[coords[:, -1]==1].mean(axis=0)[:2] / (W, H))\n",
    "\n",
    "    c_centers = np.array(c_centers)\n",
    "\n",
    "    return c_kpts, c_centers\n",
    "\n",
    "\n",
    "\n",
    "def create_labels(dirPath, annotations, dstDir):\n",
    "\n",
    "    if not os.path.exists(dstDir):\n",
    "        os.mkdir(dstDir)\n",
    "\n",
    "    trainDir = os.sep.join([dstDir, 'train'])\n",
    "    if not os.path.exists(trainDir):\n",
    "        os.mkdir(trainDir)\n",
    "\n",
    "    valDir = os.sep.join([dstDir, 'valid'])\n",
    "    if not os.path.exists(valDir):\n",
    "        os.mkdir(valDir)\n",
    "        \n",
    "\n",
    "    imgPaths = list(paths.list_images(dirPath))\n",
    "    print(f'[INFO] Found {len(annotations)} annotations...')\n",
    "    print('[INFO] Creating labels...')\n",
    "    \n",
    "    counter = 0\n",
    "    names_dict = {}\n",
    "\n",
    "    for i, sample in enumerate(annotations):\n",
    "\n",
    "        print(f'\\r[INFO] Processing image {i+1}/{len(imgPaths)}...', end=\"\")\n",
    "        imgName = sample[\"img_paths\"]\n",
    "        imgPath = os.sep.join([dirPath, imgName])\n",
    "\n",
    "        if not imgName in names_dict.keys():\n",
    "            names_dict[imgName] = 0\n",
    "        else:\n",
    "            names_dict[imgName] += 1\n",
    "            imgName = imgName.replace(\".jpg\", f\"_{names_dict[imgName]}.jpg\")\n",
    "\n",
    "        if not os.path.exists(imgPath):\n",
    "            counter+=1\n",
    "            continue\n",
    "        \n",
    "        c_keypts, c_centers = extract_coords(sample)\n",
    "        \n",
    "        dstPath = os.sep.join([valDir, imgName]) if sample['isValidation'] else os.sep.join([trainDir, imgName])\n",
    "        kptsTxtPath = dstPath.replace(\".jpg\", \"_kpts.txt\")\n",
    "        cntrsTxtPath = dstPath.replace(\".jpg\", \"_cntrs.txt\")\n",
    "\n",
    "        isLabelSaved = save_keypoints(c_keypts, kptsTxtPath)\n",
    "        isCenterSaved = save_keypoints(c_centers, cntrsTxtPath)\n",
    "        shutil.copyfile(imgPath, dstPath) \n",
    "\n",
    "        if not isLabelSaved:\n",
    "            print(f\"[ERROR] Could not save label: {kptsTxtPath}.\")\n",
    "            break\n",
    "\n",
    "        if not isCenterSaved:\n",
    "            print(f\"[ERROR] Could not save label: {cntrsTxtPath}.\")\n",
    "            break\n",
    "    print()\n",
    "    print(f'[INFO] Skipped {counter} annotations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97ab907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 25204 annotations...\n",
      "[INFO] Creating labels...\n",
      "[INFO] Processing image 25204/24984...\n",
      "[INFO] Skipped 0 annotations.\n"
     ]
    }
   ],
   "source": [
    "create_labels(\n",
    "    dirPath = \"../dataset/MPII/raw_images/\", \n",
    "    annotations = annotations, \n",
    "    dstDir = \"../dataset/MPII/images/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d865c2",
   "metadata": {},
   "source": [
    "##  3. Create TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69582f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 2958 files.\n",
      " Processing file 6/318.../10. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 14:52:15.348506: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing file 318/318...\n",
      " Processing file 306/306...0. \n",
      " Processing file 279/279...0. \n",
      " Processing file 288/288...0. \n",
      " Processing file 275/275...0. \n",
      " Processing file 284/284...0. \n",
      " Processing file 289/289...0. \n",
      " Processing file 327/327...0. \n",
      " Processing file 281/281...0. \n",
      " Processing file 311/311...10. \n"
     ]
    }
   ],
   "source": [
    "outDir = \"../dataset/tfrecords/mpii/validation/\"\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "else:\n",
    "    !rm -r $(outDit)\n",
    "\n",
    "imgPaths = list(paths.list_images(\"../dataset/MPII/images/valid/\"))\n",
    "create_TFRcords(imgPaths = imgPaths, \n",
    "                outDir = outDir + \"tfrec_val.tfrecords\", \n",
    "                target_size = (416, 416),\n",
    "                ext = \".jpg\",\n",
    "                n_splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09711d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 22246 files.\n",
      " Processing file 2253/2253... \n",
      " Processing file 2251/2251... \n",
      " Processing file 2199/2199... \n",
      " Processing file 2202/2202... \n",
      " Processing file 2237/2237... \n",
      " Processing file 2267/2267... \n",
      " Processing file 2210/2210... \n",
      " Processing file 2179/2179... \n",
      " Processing file 2235/2235... \n",
      " Processing file 2213/2213.... \n"
     ]
    }
   ],
   "source": [
    "outDir = \"../dataset/tfrecords/mpii/train/\"\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "else:\n",
    "    !rm -r $(outDit)\n",
    "\n",
    "imgPaths = list(paths.list_images(\"../dataset/MPII/images/train/\"))\n",
    "create_TFRcords(imgPaths = imgPaths, \n",
    "                outDir = outDir + \"tfrec_train.tfrecords\", \n",
    "                target_size = (416, 416),\n",
    "                ext = \".jpg\",\n",
    "                n_splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757c910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6bf254e122c73ff488b8766148b4203e9f38b207ede26a956107a11310590f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
