{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/LorBordin/bodynet/blob/master/dev_utils/training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from bodypose.training.metrics import avgMDE_2D, avgMDE_2D_RAW, Accuracy\n",
    "from bodypose.training.metrics import RegressionLoss2D, AuxiliaryLoss\n",
    "from bodypose.training.preprocessing import load_TFRecords_dataset  \n",
    "from bodypose.training.preprocessing import augmentations\n",
    "from bodypose.training.architecture import MoveNet\n",
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "STRIDES = (32, 16, 8, 4)\n",
    "NUM_KPTS = cfg.N_KPTS\n",
    "\n",
    "GRID_SIZE = INPUT_SHAPE[0] // STRIDES[-1]\n",
    "\n",
    "MODEL_PATH = f\"./saved_models/movenet_{INPUT_SHAPE[0]}.models\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = [\n",
    "    augmentations.VerticalShift(max_shift_range=.15),\n",
    "    augmentations.HorizontalShift(max_shift_range=.15),\n",
    "    augmentations.HorizontalFlip(probability=.5)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 10 TFRecords.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 15:42:25.031997: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "filePaths = list(paths.list_files(\"./dataset/tfrecords/coco/validation\"))\n",
    "np.random.shuffle(filePaths)\n",
    "print(f\"[INFO] Found {len(filePaths)} TFRecords.\")\n",
    "\n",
    "train_paths = filePaths[:-2]\n",
    "valid_path = filePaths[-2:]\n",
    "\n",
    "train_ds = load_TFRecords_dataset(\n",
    "    filePaths=train_paths, \n",
    "    batch_size = 32,\n",
    "    target_size = INPUT_SHAPE[:2],\n",
    "    grid_dim = GRID_SIZE,\n",
    "    augmentations = augmentations,\n",
    "    roi_thresh = 0.0\n",
    "    )\n",
    "\n",
    "val_ds = load_TFRecords_dataset(\n",
    "    filePaths=valid_path, \n",
    "    batch_size = 32,\n",
    "    target_size = INPUT_SHAPE[:2],\n",
    "    grid_dim = GRID_SIZE,\n",
    "    augmentations = [],\n",
    "    roi_thresh = 0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "(32, 17, 5)\n",
      "(32, 3136, 18)\n"
     ]
    }
   ],
   "source": [
    "for img, (y1, y2) in train_ds.take(1):\n",
    "    print(img.shape)\n",
    "    print(y1.shape)\n",
    "    print(y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 ms ± 639 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "model = MoveNet(\n",
    "    input_shape = INPUT_SHAPE, \n",
    "    strides = STRIDES, \n",
    "    num_joints = NUM_KPTS, \n",
    "    alpha = .25, \n",
    "    use_depthwise = True\n",
    "    )\n",
    "\n",
    "img = (np.random.uniform(\n",
    "    0, 255, (1,) + INPUT_SHAPE\n",
    "    ).astype(\"uint8\") / 255).astype(np.float32)\n",
    "\n",
    "%timeit model(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam()\n",
    "moving_avg_opt = tfa.optimizers.MovingAverage(adam)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    return  lr\n",
    "    #if epoch < 10:\n",
    "    #    return lr\n",
    "    #else:\n",
    "    #    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        MODEL_PATH,\n",
    "        monitor = \"val_avgMDE_2D\",\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "        initial_value_threshold=None,\n",
    "        ),\n",
    "    keras.callbacks.LearningRateScheduler(\n",
    "        scheduler\n",
    "        ),\n",
    "    tfa.callbacks.AverageModelCheckpoint(\n",
    "        filepath=MODEL_PATH, \n",
    "        update_weights=True\n",
    "        )\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer = moving_avg_opt,\n",
    "    loss = [RegressionLoss2D,AuxiliaryLoss],\n",
    "    loss_weights = [1., 1.],\n",
    "    metrics = [Accuracy, avgMDE_2D_RAW, avgMDE_2D],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 18s 2s/step - loss: 647562.0000 - output_1_loss: 212.9706 - output_2_loss: 647349.0000 - output_1_Accuracy: 0.2996 - output_1_avgMDE_2D_RAW: 0.7127 - output_1_avgMDE_2D: 0.4089 - output_2_Accuracy: 0.0909 - output_2_avgMDE_2D_RAW: 0.4821 - output_2_avgMDE_2D: 0.4296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ee72b80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds.take(2), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6bf254e122c73ff488b8766148b4203e9f38b207ede26a956107a11310590f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
