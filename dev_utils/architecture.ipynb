{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 10:48:47.782974: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/lorenzo/.virtualenvs/tecnobody/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from bodypose.training.metrics import avgMDE_2D, avgMDE_2D_Raw, Accuracy\n",
    "from bodypose.training.metrics import ClassificationLoss, RegrCoordsLoss\n",
    "from bodypose.training.preprocessing import load_TFRecords_dataset  \n",
    "from bodypose.training.preprocessing import augmentations\n",
    "from bodypose.training.architecture import MoveNet\n",
    "from bodypose.training.architecture.postproc import create_postproc_model\n",
    "from bodypose.demo.graphics import draw_keypoints\n",
    "from bodypose.training.architecture.custom_layers import get_max_mask\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "STRIDES = (32, 16, 8, 4)\n",
    "NUM_KPTS = len(cfg.MPII_KEYPOINT_DICT)\n",
    "\n",
    "GRID_SIZE = INPUT_SHAPE[0] // STRIDES[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 10:48:55.146479: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 10 TFRecords.\n"
     ]
    }
   ],
   "source": [
    "filePaths = list(paths.list_files(\"../dataset/tfrecords/mpii/validation/\"))\n",
    "print(f\"[INFO] Found {len(filePaths)} TFRecords.\")\n",
    "\n",
    "ds = load_TFRecords_dataset(\n",
    "    filePaths=filePaths, \n",
    "    batch_size = 10,\n",
    "    target_size = INPUT_SHAPE[:2],\n",
    "    grid_dim = GRID_SIZE,\n",
    "    augmentations = [],\n",
    "    roi_thresh = 1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 ms ± 5.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = MoveNet(\n",
    "    input_shape = INPUT_SHAPE, \n",
    "    strides = STRIDES, \n",
    "    num_joints = NUM_KPTS, \n",
    "    alpha = .5, \n",
    "    use_depthwise = True,\n",
    "    use_postproc = True\n",
    "    )\n",
    "\n",
    "img = (np.random.uniform(\n",
    "    0, 255, (1,) + INPUT_SHAPE\n",
    "    ).astype(\"uint8\") / 255).astype(np.float32)\n",
    "\n",
    "%timeit model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../saved_models/movenet_224.models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"move_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " backbone (Functional)       [(None, 7, 7, 1280),      706224    \n",
      "                              (None, 14, 14, 288),               \n",
      "                              (None, 28, 28, 96),                \n",
      "                              (None, 56, 56, 96)]                \n",
      "                                                                 \n",
      " FPN (Functional)            (None, 56, 56, 64)        113920    \n",
      "                                                                 \n",
      " head (Functional)           [(None, 56, 56, 1),       48337     \n",
      "                              (None, 56, 56, 32),                \n",
      "                              (None, 56, 56, 16),                \n",
      "                              (None, 56, 56, 32)]                \n",
      "                                                                 \n",
      " post_processing (Functional  [(None, 16, 5),          0         \n",
      " )                            (None, 56, 56, 17)]                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 868,481\n",
      "Trainable params: 848,401\n",
      "Non-trainable params: 20,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 224, 224, 3)\n",
      "*** Dataset ***\n",
      "(10, 16, 3)\n",
      "(10, 56, 56, 17)\n",
      "\n",
      "*** Model ***\n",
      "(10, 16, 5)\n",
      "(10, 56, 56, 17)\n"
     ]
    }
   ],
   "source": [
    "for img, (y1, y2) in ds.take(1):\n",
    "    coords, heatmaps = model(img)\n",
    "    print(img.shape)\n",
    "    print(\"*** Dataset ***\")\n",
    "    print(y1.shape)\n",
    "    print(y2.shape)\n",
    "    print()\n",
    "    print(\"*** Model ***\")\n",
    "    print(coords.shape)\n",
    "    print(heatmaps.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample(i):\n",
    "    sample = ((img[i].numpy() + 1) * 127.5).astype('uint8').copy()\n",
    "\n",
    "    coords, heatmaps = model(img[i:i+1])\n",
    "    #coords, heatmaps = postproc(y[i:i+1])\n",
    "    coords = coords[0].numpy()[:, [1,2,0]]\n",
    "\n",
    "    sample_pred = draw_keypoints(sample, coords, 1, cfg.MPII_KEYPOINT_DICT)\n",
    "    sample_orig = draw_keypoints(sample, y1[i].numpy()[:, [1,2,0]], .5, cfg.MPII_KEYPOINT_DICT)\n",
    "\n",
    "    colormap=cv2.COLORMAP_VIRIDIS\n",
    "    alpha = .5\n",
    "    \n",
    "    # Heatmaps\n",
    "    kptsmap = heatmaps[0, :, :, 1:].numpy().sum(axis=-1)\n",
    "    kptsmap = cv2.resize(kptsmap, INPUT_SHAPE[:2])\n",
    "    kptsmap =(kptsmap * 255).astype('uint8')\n",
    "    kptsmap = cv2.applyColorMap(kptsmap, colormap)\n",
    "\n",
    "    kptsmask = get_max_mask(heatmaps[:,:,:,1:])\n",
    "    kptsmask = kptsmask[0].numpy().sum(axis=-1)\n",
    "    kptsmask = cv2.resize(kptsmask, INPUT_SHAPE[:2])\n",
    "\n",
    "    centremap = heatmaps[0, :, :, 0].numpy()\n",
    "    centremap = cv2.resize(centremap, INPUT_SHAPE[:2])\n",
    "    centremap =(centremap * 255).astype('uint8')\n",
    "    centremap = cv2.applyColorMap(centremap, colormap)\n",
    "\n",
    "    # Labels\n",
    "    center_label = y2[i, :, :, 0].numpy()\n",
    "    center_label = cv2.resize(center_label, INPUT_SHAPE[:2])\n",
    "    center_label =(center_label * 255).astype('uint8')\n",
    "    center_label = cv2.applyColorMap(center_label, colormap)\n",
    "\n",
    "    kpts_label = y2[i, :, :, 1:].numpy().sum(axis=-1)\n",
    "    kpts_label = cv2.resize(kpts_label, INPUT_SHAPE[:2])\n",
    "    kpts_label =(kpts_label * 255).astype('uint8')\n",
    "    kpts_label = cv2.applyColorMap(kpts_label, colormap)\n",
    "    \n",
    "\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.set_figheight(20)\n",
    "    fig.set_figwidth(20)\n",
    "    \n",
    "    output = cv2.addWeighted(sample_orig, alpha, center_label, 1 - alpha, 0)\n",
    "    axs[0,0].imshow(output)\n",
    "    axs[0,0].axis('off')\n",
    "    axs[0,0].set_title(\"Label - CentreMap\")\n",
    "\n",
    "    output = cv2.addWeighted(sample_pred, alpha, centremap, 1 - alpha, 0)\n",
    "    axs[0,1].imshow(output)\n",
    "    axs[0,1].axis('off')\n",
    "    axs[0,1].set_title(\"Predicted - CentreMap\")\n",
    "\n",
    "    output = cv2.addWeighted(sample_orig, alpha, kpts_label, 1 - alpha, 0)\n",
    "    axs[1,0].imshow(output)\n",
    "    axs[1,0].axis('off')\n",
    "    axs[1,0].set_title(\"Label - KeypointsMap\")\n",
    "\n",
    "    output = cv2.addWeighted(sample_pred, alpha, kptsmap, 1 - alpha, 0)\n",
    "    axs[1,1].imshow(sample_pred, alpha=.5)\n",
    "    axs[1,1].imshow(kptsmask, alpha=.5)\n",
    "    axs[1,1].axis('off')\n",
    "    axs[1,1].set_title(\"Predicted - KeypointsMap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6bf254e122c73ff488b8766148b4203e9f38b207ede26a956107a11310590f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
